{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formal-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "allied-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>short_name</th>\n",
       "      <th>full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>e11x32M86</td>\n",
       "      <td>3K3-P11*-gfp-B15-P(ara)-sECF11*-SP1-P(Cym)-32-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2</td>\n",
       "      <td>e11x30STNpuSspS2</td>\n",
       "      <td>3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-30-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3</td>\n",
       "      <td>e11x32NpuSspS2</td>\n",
       "      <td>3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-32-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A4</td>\n",
       "      <td>e11x30NpuSspS1</td>\n",
       "      <td>3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-30-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A5</td>\n",
       "      <td>e11x32gp411</td>\n",
       "      <td>3K3-P11*-gfp-B15-P(ara)-sECF11*-SP3-P(Cym)-32-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>3K-E422</td>\n",
       "      <td>gfp_med</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>E422</td>\n",
       "      <td>gfp_low</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>C+</td>\n",
       "      <td>positive_control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>C-</td>\n",
       "      <td>negative_control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>B</td>\n",
       "      <td>blank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        short_name  \\\n",
       "0         A1         e11x32M86   \n",
       "1         A2  e11x30STNpuSspS2   \n",
       "2         A3    e11x32NpuSspS2   \n",
       "3         A4    e11x30NpuSspS1   \n",
       "4         A5       e11x32gp411   \n",
       "..       ...               ...   \n",
       "360  3K-E422           gfp_med   \n",
       "361     E422           gfp_low   \n",
       "362       C+  positive_control   \n",
       "363       C-  negative_control   \n",
       "364        B             blank   \n",
       "\n",
       "                                             full_name  \n",
       "0    3K3-P11*-gfp-B15-P(ara)-sECF11*-SP1-P(Cym)-32-...  \n",
       "1    3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-30-...  \n",
       "2    3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-32-...  \n",
       "3    3K3-P11*-gfp-B15-P(ara)-sECF11*-SP2-P(Cym)-30-...  \n",
       "4    3K3-P11*-gfp-B15-P(ara)-sECF11*-SP3-P(Cym)-32-...  \n",
       "..                                                 ...  \n",
       "360                                                NaN  \n",
       "361                                                NaN  \n",
       "362                                                NaN  \n",
       "363                                                NaN  \n",
       "364                                                NaN  \n",
       "\n",
       "[365 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constructs = pd.read_csv('datasets/constructs.csv')\n",
    "constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "embedded-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_minutes(x):\n",
    "    \n",
    "    spl = x.split(' ')\n",
    "    hours = int(spl[0]) * 60\n",
    "    mins = int(spl[2]) if spl[2] != '' else 0\n",
    "    return hours + mins\n",
    "\n",
    "def read_plate_data(df):\n",
    "    \n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def transpose_data(df):\n",
    "    \n",
    "    df.set_index('short_name', inplace=True)\n",
    "    df = df.transpose().reset_index()\n",
    "    df['time'] = df['index'].apply(parse_minutes)\n",
    "    df = df.set_index('time')\n",
    "    df = df.drop('index', axis=1)\n",
    "    return df\n",
    "\n",
    "def generate_data(df, plate, name, h=20, m=0):\n",
    "    \n",
    "    start_idx = 3\n",
    "    mid_idx = h * 3 + start_idx + 1 - m\n",
    "    end_idx = h * 3 + mid_idx + 1 - m\n",
    "\n",
    "    fluo = (df.iloc[:, start_idx:mid_idx]).astype(float)\n",
    "    od = (df.iloc[:, mid_idx:end_idx]).astype(float)\n",
    "    fluo_half = (df.iloc[:, end_idx:]).astype(float)\n",
    "    \n",
    "    fluo = pd.concat([name, fluo], axis=1)\n",
    "    od = pd.concat([name, od], axis=1)\n",
    "    fluo_half = pd.concat([name, fluo_half], axis=1)\n",
    "    \n",
    "    fluo = transpose_data(fluo)\n",
    "    od = transpose_data(od)\n",
    "    fluo_half = transpose_data(fluo_half)\n",
    "    \n",
    "    return fluo, od, fluo_half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-encoding",
   "metadata": {},
   "source": [
    "### Marionette Strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protective-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plate_map(data):\n",
    "    \n",
    "    con = data.iloc[:,:2].reset_index()\n",
    "    mar = data.iloc[:,2:].reset_index().melt(id_vars=['group'])\n",
    "    mar['variable'] = mar['variable'].apply(lambda x: \"{:02d}\".format(int(x)))\n",
    "    mar['well'] = mar['group'] + mar['variable']\n",
    "    mar.drop(['group', 'variable'], axis=1, inplace=True)\n",
    "    mar.rename(columns={'value':'id'}, inplace=True)\n",
    "    return con, mar\n",
    "\n",
    "con, mar = read_plate_map(pd.read_csv('datasets/mario_map.csv', index_col=['group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "powered-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plate_data(df):\n",
    "    \n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "data = read_plate_data(pd.read_csv('datasets/marrionette.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(data['well'], mar, on='well', how='left')\n",
    "merged = pd.merge(merged, constructs, on='id', how='left')\n",
    "name = merged['short_name'].dropna().reset_index(drop=True) #just to make sure there is no null and indexing is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_fluo, mar_od, mar_fluo_half = generate_data(data, mar, name, 24)\n",
    "gates = name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "executive-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_df(data):\n",
    "    \n",
    "    top10_wrapper = pd.DataFrame()\n",
    "    mario_wrapper = pd.DataFrame()\n",
    "\n",
    "    for gate in gates:\n",
    "\n",
    "        top10 = data[gate].iloc[:,:4]\n",
    "        top10.columns = [gate + '_00', gate + '_10', gate + '_01', gate + '_11']\n",
    "        top10_wrapper = pd.concat([top10_wrapper, top10], axis=1)\n",
    "\n",
    "        mario = data[gate].iloc[:,4:]\n",
    "        mario.columns = [gate + '_00', gate + '_10', gate + '_01', gate + '_11']\n",
    "        mario_wrapper = pd.concat([mario_wrapper, mario], axis=1)\n",
    "        \n",
    "    return top10_wrapper, mario_wrapper\n",
    "    \n",
    "top10_fluo, mario_fluo = reformat_df(mar_fluo_half)\n",
    "#top10_od, mario_od = reformat_df(mar_od)\n",
    "\n",
    "mario_fluo.reset_index().to_csv('datasets/marionette_fluo_half.csv', index=False)\n",
    "#mario_od.reset_index().to_csv('datasets/marionette_od.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-convenience",
   "metadata": {},
   "source": [
    "### ALL GATES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-iceland",
   "metadata": {},
   "source": [
    "### Data from plate reader 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-revision",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanse_plate(plate):\n",
    "    plate = plate.melt(id_vars=['Unnamed: 0'])\n",
    "    plate['value'] = plate['value'].apply(lambda x: x.split('.')[0])\n",
    "    plate['variable'] = plate['variable'].apply(lambda x: \"{:02d}\".format(int(x)))\n",
    "    plate['variable'] = plate['Unnamed: 0'] + plate['variable']\n",
    "    plate.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    return plate.reset_index(drop=True).rename(columns={'variable': 'Well', 'value': 'code_name'})\n",
    "\n",
    "plate1 = cleanse_plate(pd.read_csv('datasets/plate1_map.csv'))\n",
    "plate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = read_plate_data('and_gate_11') # up to 20h\n",
    "print(df11.shape)\n",
    "df12 = read_plate_data('and_gate_12') # up to 16h\n",
    "print(df12.shape)\n",
    "df13 = read_plate_data('and_gate_13') # up to 16h\n",
    "print(df13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df11['Well'], plate1, on='Well', how='left')\n",
    "merged = pd.merge(merged, naming_map, on='code_name', how='left')\n",
    "name1 = merged['short_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluo11, od11, bulk_fluo11, fluo_half11, bulk_fluo_half11 = generate_data(df11, plate1, name1, 20)\n",
    "fluo12, od12, bulk_fluo12, fluo_half12, bulk_fluo_half12 = generate_data(df12, plate1, name1, 16)\n",
    "fluo13, od13, bulk_fluo13, fluo_half13, bulk_fluo_half13 = generate_data(df13, plate1, name1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(data, num_row, num_col):\n",
    "    f, axs = plt.subplots(num_row, num_col, sharex=True, sharey=False, figsize=(14, num_row*2))\n",
    "    axr = axs.ravel()\n",
    "    for i, ax in tqdm(enumerate(axr)):\n",
    "        if i < data[0].shape[1]:\n",
    "            for d in data:\n",
    "                ax.plot(d.index/60, d.iloc[:, i])\n",
    "            ax.set_title(data[0].columns[i])\n",
    "            ax.set_xlabel('Time (h)')\n",
    "        else:\n",
    "            ax.set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    \n",
    "#plot bulk fluorescence data\n",
    "plot_all([bulk_fluo_half11, bulk_fluo_half12, bulk_fluo_half13], 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all([od11, od12, od13], 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo1 = (bulk_fluo11 + bulk_fluo12 + bulk_fluo13) / 3\n",
    "fluo1 = (fluo11 + fluo12 + fluo13) / 3\n",
    "od1 = (od11 + od12 + od13) / 3\n",
    "fluo_half1 = (fluo_half11 + fluo_half12 + fluo_half13) / 3\n",
    "bulk_fluo_half1 = (bulk_fluo_half11 + bulk_fluo_half12 + bulk_fluo_half13) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo1.dropna().to_csv('datasets/bulk_fluo_plate_1_triplicate.csv')\n",
    "fluo1.dropna().to_csv('datasets/fluo_plate_1_triplicate.csv')\n",
    "od1.dropna().to_csv('datasets/od_plate_1_triplicate.csv')\n",
    "bulk_fluo_half1.dropna().to_csv('datasets/bulk_fluo_half_plate_1_triplicate.csv')\n",
    "fluo_half1.dropna().to_csv('datasets/fluo_half_plate_1_triplicate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo11.dropna().to_csv('datasets/bulk_fluo_plate_1_single.csv')\n",
    "fluo11.dropna().to_csv('datasets/fluo_plate_1_single.csv')\n",
    "od11.dropna().to_csv('datasets/od_plate_1_single.csv')\n",
    "bulk_fluo_half11.dropna().to_csv('datasets/bulk_fluo_half_plate_1_single.csv')\n",
    "fluo_half11.dropna().to_csv('datasets/fluo_half_plate_1_single.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-alberta",
   "metadata": {},
   "source": [
    "### Data from plate reader 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate2 = cleanse_plate(pd.read_csv('datasets/plate2_map.csv'))\n",
    "df21 = read_plate_data('and_gate_21') # up to 20h\n",
    "print(df21.shape)\n",
    "df22 = read_plate_data('and_gate_22') # up to 16h\n",
    "print(df22.shape)\n",
    "df23 = read_plate_data('and_gate_23') # up to 16h\n",
    "print(df23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df21['Well'], plate2, on='Well', how='left')\n",
    "merged = pd.merge(merged, naming_map, on='code_name', how='left')\n",
    "name2 = merged['short_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluo21, od21, bulk_fluo21, fluo_half21, bulk_fluo_half21 = generate_data(df21, plate2, name2, 21, 2)\n",
    "fluo22, od22, bulk_fluo22, fluo_half22, bulk_fluo_half22 = generate_data(df22, plate2, name2, 22, 0)\n",
    "fluo23, od23, bulk_fluo23, fluo_half23, bulk_fluo_half23 = generate_data(df23, plate2, name2, 22, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all([bulk_fluo_half21, bulk_fluo_half22, bulk_fluo_half23], 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all([od21, od22, od23], 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo2 = (bulk_fluo21 + bulk_fluo22 + bulk_fluo23) / 3\n",
    "fluo2 = (fluo21 + fluo22 + fluo23) / 3\n",
    "od2 = (od21 + od22 + od23) / 3\n",
    "fluo_half2 = (fluo_half21 + fluo_half22 + fluo_half23) / 3\n",
    "bulk_fluo_half2 = (bulk_fluo_half21 + bulk_fluo_half22 + bulk_fluo_half23) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo2.to_csv('datasets/bulk_fluo_plate_2_triplicate.csv')\n",
    "fluo2.to_csv('datasets/fluo_plate_2_triplicate.csv')\n",
    "fluo_half2.to_csv('datasets/fluo_half_plate_2_triplicate.csv')\n",
    "bulk_fluo_half2.to_csv('datasets/bulk_fluo_half_plate_2_triplicate.csv')\n",
    "od2.to_csv('datasets/od_plate_2_triplicate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo21.to_csv('datasets/bulk_fluo_plate_2_single.csv')\n",
    "fluo21.to_csv('datasets/fluo_plate_2_single.csv')\n",
    "fluo_half21.to_csv('datasets/fluo_half_plate_2_single.csv')\n",
    "bulk_fluo_half21.to_csv('datasets/bulk_fluo_half_plate_2_single.csv')\n",
    "od21.to_csv('datasets/od_plate_2_single.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-cinema",
   "metadata": {},
   "source": [
    "### Induction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "induction = read_plate_data('induction') # up to 20h\n",
    "print(induction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_plate = {\n",
    "    'A01': 'A18', 'A02': 'A18', 'A03': 'A18', \n",
    "    'B01': 'A18', 'B02': 'A18', 'B03': 'A18', \n",
    "    'C01': 'A18', 'C02': 'A18', 'C03': 'A18', \n",
    "    'D01': 'A195', 'D02': 'A195', 'D03': 'A195', \n",
    "    'E01': 'A195', 'E02': 'A195', 'E03': 'A195', \n",
    "    'F01': 'A195', 'F02': 'A195', 'F03': 'A195', \n",
    "    'A04': 'A29', 'A05': 'A29', 'A06': 'A29', \n",
    "    'B04': 'A29', 'B05': 'A29', 'B06': 'A29', \n",
    "    'C04': 'A29', 'C05': 'A29', 'C06': 'A29', \n",
    "    'D04': 'A259', 'D05': 'A259', 'D06': 'A259', \n",
    "    'E04': 'A259', 'E05': 'A259', 'E06': 'A259', \n",
    "    'F04': 'A259', 'F05': 'A259', 'F06': 'A259', \n",
    "    'A07': 'A76', 'A08': 'A76', 'A09': 'A76', \n",
    "    'B07': 'A76', 'B08': 'A76', 'B09': 'A76', \n",
    "    'C07': 'A76', 'C08': 'A76', 'C09': 'A76', \n",
    "    'D07': 'A267', 'D08': 'A267', 'D09': 'A267', \n",
    "    'E07': 'A267', 'E08': 'A267', 'E09': 'A267', \n",
    "    'F07': 'A267', 'F08': 'A267', 'F09': 'A267', \n",
    "    'A10': 'A109', 'A11': 'A109', 'A12': 'A109', \n",
    "    'B10': 'A109', 'B11': 'A109', 'B12': 'A109', \n",
    "    'C10': 'A109', 'C11': 'A109', 'C12': 'A109', \n",
    "    'D10': 'A294', 'D11': 'A294', 'D12': 'A294', \n",
    "    'E10': 'A294', 'E11': 'A294', 'E12': 'A294', \n",
    "    'F10': 'A294', 'F11': 'A294', 'F12': 'A294', \n",
    "    'G01': 'A323', 'G02': 'A323', 'G03': 'A323', \n",
    "    'G04': 'A323', 'G05': 'A323', 'G06': 'A323', \n",
    "    'H01': 'A323', 'H02': 'A323', 'H03': 'A323', \n",
    "}\n",
    "induction_row = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 0, 'E': 1, 'F': 2,\n",
    "    'G': 0, 'H': 1 #column g needs to be manually adjusted later\n",
    "}\n",
    "induction_col = {\n",
    "    '01': 0, '02': 1, '03': 2, '04': 0, '05': 1, '06': 2, \n",
    "    '07': 0, '08': 1, '09': 2, '10': 0, '11': 1, '12': 2 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = induction.iloc[:,:3]\n",
    "index['code_name'] = index['Well'].map(induction_plate)\n",
    "col_idx = index['Well'].str[1:]\n",
    "row_idx = index['Well'].str[:1]\n",
    "index['ind1_lvl'] = row_idx.map(induction_row)\n",
    "index['ind2_lvl'] = col_idx.map(induction_col)\n",
    "index.loc[index['Well'].isin(['G04', 'G05', 'G06']), 'ind1_lvl'] = 2\n",
    "#index = index.dropna()\n",
    "index.loc[index['code_name'].isnull(), 'code_name'] = 'control'\n",
    "name = pd.merge(index[['code_name', 'ind1_lvl', 'ind2_lvl']], naming_map[['code_name', 'short_name']], on='code_name', how='left')\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "index[index['code_name']=='A323']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_minutes(x):\n",
    "    spl = x.split(' ')\n",
    "    hours = int(spl[0]) * 60\n",
    "    mins = int(spl[2]) if spl[2] != '' else 0\n",
    "    return hours + mins\n",
    "\n",
    "def transpose_data(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df.set_index('short_name', inplace=True)\n",
    "    #df = df.transpose().reset_index()\n",
    "    #df['time'] = df['index'].apply(parse_minutes)\n",
    "    #df = df.set_index('time')\n",
    "    #df = df.drop('index', axis=1)\n",
    "    return df\n",
    "\n",
    "def generate_data_induce(df, h=24):\n",
    "    \n",
    "    start_idx = 3\n",
    "    mid_idx = h * 3 + start_idx\n",
    "    fluo = (df.iloc[:, start_idx:mid_idx]).astype(float)\n",
    "    od = (df.iloc[:, mid_idx:]).astype(float)\n",
    "    bulk_fluo = fluo * od\n",
    "    \n",
    "    fluo = pd.concat([name, fluo], axis=1)\n",
    "    od = pd.concat([name, od], axis=1)\n",
    "    bulk_fluo = pd.concat([name, bulk_fluo], axis=1)\n",
    "    \n",
    "    #bulk_fluo = transpose_data(bulk_fluo)\n",
    "    #fluo = transpose_data(fluo)\n",
    "    #od = transpose_data(od)\n",
    "    \n",
    "    return fluo, od, bulk_fluo\n",
    "\n",
    "fluo, od, bulk_fluo = generate_data_induce(induction)\n",
    "bulk_fluo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo[bulk_fluo['short_name']=='e11x32STPhoRadA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fluo.to_csv('datasets/bulk_fluo_induction.csv', index=False)\n",
    "fluo.to_csv('datasets/fluo_induction.csv', index=False)\n",
    "od.to_csv('datasets/od_induction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-british",
   "metadata": {},
   "source": [
    "### XOR Gate Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plate_map(df):\n",
    "    data = pd.read_csv('datasets/xor_map.csv', index_col=['Group'])\n",
    "    con = data.iloc[:,:2].reset_index()\n",
    "    xor = data.iloc[:,2:]\n",
    "    xor = xor.reset_index().melt(id_vars=['Group'])\n",
    "    xor['variable'] = xor['variable'].apply(lambda x: \"{:02d}\".format(int(x)))\n",
    "    xor['Well'] = xor['Group'] + xor['variable']\n",
    "    xor.drop(['Group', 'variable'], axis=1, inplace=True)\n",
    "    xor.rename(columns={'value':'code_name'}, inplace=True)\n",
    "    return con, xor\n",
    "\n",
    "con, xor = read_plate_map(pd.read_csv('datasets/xor_map.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plate_data(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "    #df.dropna(inplace=True)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "data = read_plate_data(pd.read_csv('datasets/xor_gate.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(data['Well'], xor, on='Well', how='left')\n",
    "#merged = pd.merge(merged, naming_map, on='code_name', how='left')\n",
    "merged.rename(columns={'code_name': 'short_name'}, inplace=True)\n",
    "name = merged['short_name'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluox, odx, fluo_halfx = generate_data(data, xor, name, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ECF20/33', 'ECF11/15']\n",
    "fluo_xor = pd.DataFrame()\n",
    "od_xor = pd.DataFrame()\n",
    "for c in cols:\n",
    "    temp = pd.concat([fluox[c].iloc[:,i] for i in range(0, 16, 4)], axis=1)\n",
    "    temp.columns = [c + '_' + \"{:02b}\".format(int(i)) for i in range(4)]\n",
    "    fluo_xor = pd.concat([fluo_xor, temp], axis=1)\n",
    "    \n",
    "    temp = pd.concat([odx[c].iloc[:,i] for i in range(0, 16, 4)], axis=1)\n",
    "    temp.columns = [c + '_' + \"{:02b}\".format(int(i)) for i in range(4)]\n",
    "    od_xor = pd.concat([od_xor, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluo_xor.to_csv('datasets/bulk_fluo_xor.csv')\n",
    "od_xor.to_csv('datasets/od_xor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-sweden",
   "metadata": {},
   "source": [
    "### First-round Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/raw.csv')\n",
    "df.columns = df.iloc[0]\n",
    "df.drop(df.index[0], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ecf = {'Sample X1': 'e15',\n",
    "'Sample X2': 'e22',\n",
    "'Sample X3': 'e32',\n",
    "'Sample X4': 'e33',\n",
    "'Sample X5': 'e34',\n",
    "'Sample X6': 'e41',\n",
    "'Sample X7': 'e42',\n",
    "'Sample X8': 'e15',\n",
    "'Sample X9': 'e22',\n",
    "'Sample X10': 'e38',\n",
    "'Sample X11': 'e16',\n",
    "'Sample X12': 'e33',\n",
    "'Sample X13': 'e15',\n",
    "'Sample X14': 'e16',\n",
    "'Sample X15': 'e17',\n",
    "'Sample X16': 'e20',\n",
    "'Sample X17': 'e22',\n",
    "'Sample X18': 'e26',\n",
    "'Sample X19': 'e32',\n",
    "'Sample X20': 'e33',\n",
    "'Sample X21': 'e34'}\n",
    "map_int = {'Sample X1': 'SspGyrB',\n",
    "'Sample X2': 'SspGyrB',\n",
    "'Sample X3': 'SspGyrB',\n",
    "'Sample X4': 'SspGyrB',\n",
    "'Sample X5': 'SspGyrB',\n",
    "'Sample X6': 'SspGyrB',\n",
    "'Sample X7': 'SspGyrB',\n",
    "'Sample X8': 'TerThyXS2',\n",
    "'Sample X9': 'TerThyXS2',\n",
    "'Sample X10': 'TerThyXS2',\n",
    "'Sample X11': 'TerThyXS1',\n",
    "'Sample X12': 'TerThyXS1',\n",
    "'Sample X13': 'STPhoRadA',\n",
    "'Sample X14': 'STPhoRadA',\n",
    "'Sample X15': 'STPhoRadA',\n",
    "'Sample X16': 'STPhoRadA',\n",
    "'Sample X17': 'STPhoRadA',\n",
    "'Sample X18': 'STPhoRadA',\n",
    "'Sample X19': 'STPhoRadA',\n",
    "'Sample X20': 'STPhoRadA',\n",
    "'Sample X21': 'STPhoRadA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ECF'] = df['Content'].map(map_ecf)\n",
    "df['Intein'] = df['Content'].map(map_int)\n",
    "df.loc[df['Group'].isin(['A', 'E']), 'Ara'] = 0\n",
    "df.loc[df['Group'].isin(['A', 'E']), 'Cuma'] = 0\n",
    "df.loc[df['Group'].isin(['B', 'F']), 'Ara'] = 1\n",
    "df.loc[df['Group'].isin(['B', 'F']), 'Cuma'] = 0\n",
    "df.loc[df['Group'].isin(['C', 'G']), 'Ara'] = 0\n",
    "df.loc[df['Group'].isin(['C', 'G']), 'Cuma'] = 1\n",
    "df.loc[df['Group'].isin(['D', 'H']), 'Ara'] = 1\n",
    "df.loc[df['Group'].isin(['D', 'H']), 'Cuma'] = 1\n",
    "df = pd.concat([df[['ECF', 'Intein', 'Ara', 'Cuma']],\n",
    "                 df.drop(['ECF', 'Intein', 'Ara', 'Cuma', 'Well', 'Content', 'Group'], axis=1)],\n",
    "                 axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(range(0,77))\n",
    "fluo = df.iloc[:, cols]\n",
    "#fluo.to_csv('datasets/fluoOD-all.csv', index=False)\n",
    "fluo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(range(0,4)) + list(range(77,150))\n",
    "od = df.iloc[:, cols]\n",
    "#od.to_csv('datasets/OD-all.csv', index=False)\n",
    "od.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_minutes(x):\n",
    "    spl = x.split(' ')\n",
    "    hours = int(spl[0]) * 60\n",
    "    mins = int(spl[2]) if spl[2] != '' else 0\n",
    "    return hours + mins\n",
    "\n",
    "fluo11 = fluo[(fluo['Ara']==1) & (fluo['Cuma']==0)]\n",
    "fluo11['index'] = fluo11['ECF'] + fluo11['Intein']\n",
    "fluo11.set_index('index', inplace=True)\n",
    "fluo11.drop(['ECF', 'Intein', 'Ara', 'Cuma'], axis=1, inplace=True)\n",
    "fluo11 = fluo11.transpose().reset_index()\n",
    "\n",
    "fluo11['time'] = fluo11[0].apply(parse_minutes)\n",
    "fluo11 = fluo11.set_index('time')\n",
    "fluo11 = fluo11.drop(0, axis=1)\n",
    "fluo11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "od11 = od[(od['Ara']==1) & (od['Cuma']==0)]\n",
    "od11['index'] = od11['ECF'] + od11['Intein']\n",
    "od11.set_index('index', inplace=True)\n",
    "od11.drop(['ECF', 'Intein', 'Ara', 'Cuma'], axis=1, inplace=True)\n",
    "od11 = od11.transpose().reset_index()\n",
    "\n",
    "od11['time'] = od11[0].apply(parse_minutes)\n",
    "od11 = od11.set_index('time')\n",
    "od11 = od11.drop(0, axis=1)\n",
    "od11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in fluo11.columns:\n",
    "    try:\n",
    "        fluo11[col] = fluo11[col].astype(float)\n",
    "    except:\n",
    "        print(col)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in od11.columns:\n",
    "    od11[col] = od11[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluo11.to_csv('datasets/fluo-10.csv')\n",
    "od11.to_csv('datasets/od-10.csv')\n",
    "(fluo11 * od11).to_csv('datasets/bulk-fluo-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-arbitration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
